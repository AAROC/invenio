##Ranking of records using different parameters and methods on the fly.

## This file is part of the CERN Document Server Software (CDSware).
## Copyright (C) 2002 CERN.
##
## The CDSware is free software; you can redistribute it and/or
## modify it under the terms of the GNU General Public License as
## published by the Free Software Foundation; either version 2 of the
## License, or (at your option) any later version.
##
## The CDSware is distributed in the hope that it will be useful, but
## WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
## General Public License for more details.
##
## You should have received a copy of the GNU General Public License
## along with CDSware; if not, write to the Free Software Foundation, Inc.,
## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

## read config variables:
#include "config.wml"
#include "configbis.wml"
#include "cdswmllib.wml"

## start Python:
<protect>#!</protect><PYTHON>
<protect># -*- coding: utf-8 -*-</protect>
<protect>## $Id$</protect>
<protect>## DO NOT EDIT THIS FILE!  IT WAS AUTOMATICALLY GENERATED FROM CDSware WML SOURCES.</protect>
## fill config variables:
pylibdir = "<LIBDIR>/python"

try:
    import zlib
    import marshal
    import string
    import sys
    import time
    import math
    import MySQLdb
    import Numeric
    import re
    import ConfigParser
except ImportError, e:
    import sys

try:
    import Stemmer
except ImportError, e:
    pass

try:
    sys.path.append('%s' % pylibdir)
    from cdsware.config import *
    from cdsware.dbquery import run_sql
    from search_engine_config import *
except ImportError, e:
    import sys

class HitSet:
    """Class describing set of records, implemented as bit vectors of recIDs.
    Using Numeric arrays for speed (1 value = 8 bits), can use later "real"
    bit vectors to save space."""

    def __init__(self, init_set=None):
        self._nbhits = -1
        if init_set:
            self._set = init_set
        else:
            self._set = Numeric.zeros(cfg_max_recID+1, Numeric.Int0)

    def __repr__(self, join=string.join):
        return "%s(%s)" % (self.__class__.__name__, join(map(repr, self._set), ', '))

    def add(self, recID):
        "Adds a record to the set."
        self._set[recID] = 1

    def addmany(self, recIDs):
        "Adds several recIDs to the set."
        for recID in recIDs: self._set[recID] = 1

    def addlist(self, arr):
        "Adds an array of recIDs to the set."
        Numeric.put(self._set, arr, 1)

    def remove(self, recID):
        "Removes a record from the set."
        self._set[recID] = 0

    def removemany(self, recIDs):
        "Removes several records from the set."
        for recID in recIDs:
            self.remove(recID)

    def intersect(self, other):
        "Does a set intersection with other.  Keep result in self."
        self._set = Numeric.bitwise_and(self._set, other._set)

    def union(self, other):
        "Does a set union with other. Keep result in self."
        self._set = Numeric.bitwise_or(self._set, other._set)

    def difference(self, other):
        "Does a set difference with other. Keep result in self."
        #self._set = Numeric.bitwise_not(self._set, other._set)
        for recID in Numeric.nonzero(other._set):
            self.remove(recID)

    def contains(self, recID):
        "Checks whether the set contains recID."
        return self._set[recID]

    __contains__ = contains     # Higher performance member-test for python 2.0 and above

    def __getitem__(self, index):
        "Support for the 'for item in set:' protocol."
        return Numeric.nonzero(self._set)[index]
        
    def calculate_nbhits(self):
        "Calculates the number of records set in the hitset."
        self._nbhits = Numeric.sum(self._set.copy().astype(Numeric.Int))

    def items(self):
        "Return an array containing all recID."
        return Numeric.nonzero(self._set)

    def tolist(self):
        "Return an array containing all recID."
        return Numeric.nonzero(self._set).tolist()

def compare_on_val(first, second):
    return cmp(second[1], first[1])
def serialize_via_numeric_array_dumps(arr):
    return Numeric.dumps(arr)
def serialize_via_numeric_array_compr(str):
    return zlib.compress(str)
def serialize_via_numeric_array_escape(str):
    return MySQLdb.escape_string(str)
def serialize_via_numeric_array(arr):
    """Serialize Numeric array into a compressed string."""
    return serialize_via_numeric_array_escape(serialize_via_numeric_array_compr(serialize_via_numeric_array_dumps(arr)))
def deserialize_via_numeric_array(string):
    """Decompress and deserialize string into a Numeric array."""
    return Numeric.loads(zlib.decompress(string))
def serialize_via_marshal(obj):
    """Serialize Python object via marshal into a compressed string."""
    return MySQLdb.escape_string(zlib.compress(marshal.dumps(obj)))
def deserialize_via_marshal(string):
    """Decompress and deserialize string into a Python object via marshal."""
    return marshal.loads(zlib.decompress(string))

def check_term(stopwords, term, col_size, term_rec, max_occ, min_occ, termlength):
    """Check if the term is valid for use
    stopwords - a dict of stopwords
    term - the term to check
    col_size - the number of records in database
    term_rec - the number of records which contains this term
    max_occ - max frequency of the term allowed
    min_occ - min frequence of the term allowed
    termlength - the minimum length of the terms allowed"""
    try:
        if stopwords.has_key(term) or (len(term) <= termlength) or ((float(term_rec) / float(col_size)) >= max_occ) or ((float(term_rec) / float(col_size)) <= min_occ):
	    return ""
        if int(term):
            return ""
    except StandardError, e:
	pass
    return "true"

def get_stopwords(file='stopwords.kb'):
    """Load the stopwords from the file and returns a dict cotaining the words."""
    stopwords = open(file, 'r')
    lines = stopwords.readlines()
    stopwords.close()
    stopwords = {}
    for line in lines:
        stopwords[string.rstrip(line)] = 1
    return stopwords

def get_config(rank_method_code): #needs some work
    """Load common data into memory"""
    global stopwords
    global stemmer
    global chars_alphanumericseparators
    global col_size
    global rnkWORD_table
    languages = {'fr': 'french', 'en': 'porter', 'no':'norwegian', 'se':'swedish', 'de': 'german', 'it':'italian', 'pt':'portugese'}

    try: 
        if stemmer and stopwords:     
            pass
    except StandardError, e:
        rnkWORD_table = methods[rank_method_code]["rnkWORD_table"]
        try:
            if methods[rank_method_code].has_key("stem_lang"):
                stemmer = Stemmer.Stemmer(languages[methods[rank_method_code]["stem_lang"]])
            else: 
                stemmer = None         
        except Exception, e:
            stemmer = None
        if methods[rank_method_code].has_key("stopword"):
            stopwords = get_stopwords("%s" % methods[rank_method_code]["stopword"]) 
        else:
            stopwords = {}
        chars_alphanumericseparators = r"[1234567890\!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~]"
        col_size = run_sql("SELECT count(*) FROM %sR" % rnkWORD_table[:-1])[0][0]

def create_rnkmethod_cache():
    """Create cache with vital information for each rank method."""

    global methods
    bibrank_meths = run_sql("SELECT name from rnkMETHOD")
    methods = {}
    for (rank_method_code,) in bibrank_meths:
        try:
            file = etcdir + "/bibrank/" + rank_method_code + ".cfg"
            config = ConfigParser.ConfigParser()
            config.readfp(open(file))
        except StandardError, e:
            pass
        cfg_function = config.get("rank_method", "function")
        methods[rank_method_code] = {}
        methods[rank_method_code]["function"] = cfg_function
        if config.has_option(cfg_function, "table"):
            methods[rank_method_code]["rnkWORD_table"] = config.get(cfg_function, "table")
        if  config.has_option(cfg_function, "stem_if_avail") and config.get(cfg_function, "stem_if_avail") == "yes":
            methods[rank_method_code]["stem_lang"] = config.get(cfg_function, "stem_query_language")

        if config.has_option(cfg_function, "stopword"):
            methods[rank_method_code]["stopword"] = config.get(cfg_function, "stopword")
        if config.has_section("find_similar"):
            methods[rank_method_code]["max_word_occurence"] = float(config.get("find_similar", "max_word_occurence"))
            methods[rank_method_code]["min_word_occurence"] = float(config.get("find_similar", "min_word_occurence"))
            methods[rank_method_code]["min_word_length"] = int(config.get("find_similar", "min_word_length"))
            methods[rank_method_code]["min_nr_words_docs"] = int(config.get("find_similar", "min_nr_words_docs"))
            methods[rank_method_code]["max_nr_words_upper"] = int(config.get("find_similar", "max_nr_words_upper"))
            methods[rank_method_code]["max_nr_words_lower"] = int(config.get("find_similar", "max_nr_words_lower"))
            methods[rank_method_code]["override_default_min_relevance"] = config.get("find_similar", "override_default_min_relevance")
            methods[rank_method_code]["default_min_relevance"] = int(config.get("find_similar", "default_min_relevance"))

        i8n_names = run_sql("SELECT ln,value from rnkMETHODNAME,rnkMETHOD where id_rnkMETHOD=rnkMETHOD.id and rnkMETHOD.name='%s'" % (rank_method_code))
        for (ln, value) in i8n_names:
            methods[rank_method_code][ln] = value
            
def get_bibrank_methods(ln=cdslang):
    """Returns a list of rank methods and the name om them in the language defined by the ln parameter"""

    try:
        if methods:
            pass
    except Exception:
        create_rnkmethod_cache()

    avail_methods = []
    for (rank_method_code, options) in methods.iteritems():
        if options.has_key("function"): #and is_method_valid(collection, rank_method_code):
            if options.has_key(ln):
                avail_methods.append((rank_method_code, options[ln]))
            else:
                avail_methods.append((rank_method_code, "Not translated"))              
    return avail_methods

def rank_records(rank_method_code, rank_limit_relevance, lrecIDs=None, pattern="", verbose=0):
    """rank_method, e.g. `jif' or `sbr' (word frequency vector model)                    
       rank_limit_relevance, e.g. `23' for `nbc' (number of citations) or `0.10' for `vec'                   
       hitset, search engine hits; optional                   
       pattern, search engine query or record ID (you check the type)                   
       verbose, verbose level
    """

    try:
        if methods:
            pass
    except Exception:
        create_rnkmethod_cache()

    #try:
    if 1:
        function = methods[rank_method_code]["function"]
        func_object = globals().get(function)
        if func_object and string.find(pattern[0], "recid:") > -1:
            get_config(rank_method_code)
            result = find_similar(rank_method_code, pattern[0][6:], lrecIDs, rank_limit_relevance, verbose)
        elif func_object:
            get_config(rank_method_code)
            result = func_object(rank_method_code, pattern, lrecIDs, rank_limit_relevance, verbose)
        else:
            result = rank_by_method(rank_method_code, pattern, lrecIDs, rank_limit_relevance, verbose)
        return result
    #except StandardError, e:
    #    return []

def find_similar(rank_method_code, recID, lrecIDs, rank_limit_relevance=10,verbose=0):
    """Finding terms to use for calculating similarity. Terms are taken from the recid given, returns a list of recids's and relevance, [[23,34], [344,24], [1,01]]
    recID - record to use for find similar
    rank_limit_relevance - find all similar document above given percentage (0-100)
    verbose - how much debug information to show, 0-9"""
    
    startCreate = time.time()
    if methods[rank_method_code]["override_default_min_relevance"] == "no":
        rank_limit_relevance = methods[rank_method_code]["default_min_relevance"]

    query_terms = {}
    recID = int(recID)
    if type(recID) != int:
    	return []

    res = run_sql("SELECT id_bibrec, termlist FROM %sR WHERE id_bibrec=%s" % (rnkWORD_table[:-1], recID))
    if not res:
        return []

    rec_terms = deserialize_via_marshal(res[0][1])
    #Get all documents using terms from the selected documents
    if len(rec_terms) > 0:
        terms = "%s" % dict(rec_terms).keys()
        terms = terms[1:len(terms) - 1]
        terms_recs = dict(run_sql("SELECT term, hitlist FROM %s WHERE term IN (%s)" % (rnkWORD_table,terms)))
    else:
        return []

    #Calculate all terms
    for (term, tf) in rec_terms.iteritems():
	if len(term) >= methods[rank_method_code]["min_word_length"] and terms_recs.has_key(term):
            query_terms[term] =  (1 + math.log(tf[0])) *  tf[1]
    #if len(query_terms) < methods[rank_method_code]["min_nr_words"]:
    #    return []

    query_terms_old = query_terms.items()
    query_terms_old.sort(lambda x, y: cmp(y[1], x[1])) 
    query_terms = {}

    stime = time.time()#
    (recdict, rec_termcount, lrecIDs_remove) = ({}, {}, {})
    #Use only most important terms
    for (term, tf) in query_terms_old:
        term_recs = deserialize_via_marshal(terms_recs[term])
        #to be fixed
        if len(query_terms_old) <= methods[rank_method_code]["max_nr_words_lower"] or (len(term_recs) >= methods[rank_method_code]["min_nr_words_docs"] and (((float(len(term_recs)) / float(col_size)) <=  methods[rank_method_code]["max_word_occurence"]) and ((float(len(term_recs)) / float(col_size)) >= methods[rank_method_code]["min_word_occurence"]))):
             query_terms[term] = round(tf, 4)
             (recdict, rec_termcount, lrecIDs_remove) = calculate_record_relevance((term, query_terms[term]) , term_recs, None, recdict, rec_termcount, lrecIDs_remove, verbose) 
        if len(query_terms_old) > methods[rank_method_code]["max_nr_words_lower"] and (len(query_terms) ==  methods[rank_method_code]["max_nr_words_upper"] or tf < 0):
            break

    if len(recdict) == 0:
        return []

    if verbose > 0:
        print "Number of terms: %s" % run_sql("SELECT count(id) FROM %s" % rnkWORD_table)[0][0]
        print "Number of terms to use for query: %s" % (len(query_terms))
        print "Current number of recIDs: %s" % (col_size)
        print "Terms to use: %s" % query_terms
        print "Prepare time: %s" % (str(time.time() - startCreate))

    recdict = post_calculate_record_relevance(recdict, rec_termcount, lrecIDs_remove, None, verbose)
    reclist = sort_record_relevance(recdict, rank_limit_relevance,recID, verbose)

    i = 0
    if len(reclist) > 30:
        w = reclist[len(reclist) - 2][1] / 5
        i = len(reclist) - 1
        while reclist[i][1] > w and i > 0:
            i -= 1
        if len(reclist) - 30 < i:
            i = len(reclist) - 30

    if verbose == 9:
        stat(reclist, query_terms)
    #return reclist[i:len(reclist)]
    return (reclist[i:len(reclist)], "(", "%)")

def rank_by_method(lwords, lrecIDs, rank_limit_relevance,rank_method_code,verbose=0):
    """input: list of words, ['ellis', 'muon']          
    optional list of recIDs
    output: sorted list of recIDs based on rank method given, e.g. [[23,34], [344,24], [1,01]]           
    if not possible, then return empty list""" 

    rnkdict = run_sql("SELECT relevance_data FROM rnkMETHODDATA,rnkMETHOD where rnkMETHOD.id=id_rnkMETHOD and rnkMETHOD.name='%s'" % rank_method_code)
    rnkdict = deserialize_via_marshal(rnkdict[0][0])
    lrecIDs = lrecIDs.items()
    reclist = []
    reclist_addend = []

    for recID in lrecIDs:
        if rnkdict.has_key(recID):
            reclist.append((recID, rnkdict[recID][1]))
            del rnkdict[recID]
        else:
            reclist_addend.append((recID, 0))
    reclist.sort(lambda x, y: cmp(x[1], y[1]))
    return (reclist_addend + reclist)
    #return (reclist_addend + reclist, "", "")

def word_frequency(rank_method_code, lwords, lrecIDs, rank_limit_relevance,verbose=0):
    """input: list of words, ['ellis', 'muon']          
    optional list of recIDs
    output: sorted list of recIDs by summary word frequencies, e.g. [[23,34], [344,24], [1,01]]           
    if not possible (e.g. all stopwords), then return empty list""" 

    startCreate = time.time()
    query_terms = {}
    lwords_old = lwords
    lwords = []
 
    #Check terms, remove non alphanumeric characters. Use both unstemmed and stemmed version of all terms.
    for i in range(0, len(lwords_old)):
        term = lwords_old[i]
        use_term = 0
        if not stopwords.has_key(term):
            lwords.append(term)
            terms = string.split(string.lower(re.sub(chars_alphanumericseparators, ' ', term)))  
            for term in terms: 
                if stemmer: # stem word
                    term = stemmer.stem(string.replace(term, ' ', ''))
                if lwords_old[i] != term: #add if stemmed word is different than original word
	            lwords.append(term)

    (recdict, rec_termcount, lrecIDs_remove) = ({}, {}, {})
    #For each term, if accepted, get a list of the records using the term
    #calculate then relevance for each term before sorting the list of records
    for term in lwords:
        term = string.lower(term)
	term_recs = run_sql("SELECT term, hitlist FROM %s WHERE term='%s'" % (rnkWORD_table,  MySQLdb.escape_string(term)))
        if term_recs:
	    term_recs = deserialize_via_marshal(term_recs[0][1])
            if check_term({}, term, col_size, len(term_recs), 1.0, 0.00, 0):
                query_terms[term] = query_terms.get(term, 0) + term_recs["Gi"][1]
                (recdict, rec_termcount, lrecIDs_remove) = calculate_record_relevance((term, query_terms[term]) , term_recs, lrecIDs, recdict, rec_termcount, lrecIDs_remove, verbose)
            del term_recs

    if len(recdict) == 0 or len(lwords) == 1 and lwords[0] == "":
        return []

    if verbose > 0:
        print "Current number of recIDs: %s" % (col_size)
        print "Number of terms: %s" % run_sql("SELECT count(id) FROM %s" % rnkWORD_table)[0][0]
        print "Terms: %s" % query_terms
        print "Prepare and calculate time: %s" % (str(time.time() - startCreate))

    recdict = post_calculate_record_relevance(recdict, rec_termcount, lrecIDs_remove, lrecIDs, verbose)
    reclist = sort_record_relevance(recdict, rank_limit_relevance, 0, verbose)
    
    #Add any documents not ranked to the end of the list
    if lrecIDs:
        lrecIDs.calculate_nbhits()
    if lrecIDs and len(reclist) < lrecIDs._nbhits: #add records found but not in list
        lrecIDs = lrecIDs.tolist()                                #using 2-3mb 
        reclist = zip(lrecIDs, [0] * len(lrecIDs)) + reclist      #using 6mb

    if verbose == 9:
        print query_terms
        print col_size
        stat(reclist, query_terms)
    #return reclist
    return (reclist, "(", "%)")

def calculate_record_relevance(term, invidx, lrecIDs, recdict, rec_termcount, lrecIDs_remove, verbose):
    """Calculating the relevance of the documents based on the input"""

    startCreate = time.time()
    (t, qtf) = term
    Gi = invidx["Gi"][1]
    del invidx["Gi"]

    if lrecIDs:
        #Only accept records existing in the hitset received from the search engine
        for (j, tf) in invidx.iteritems():
            if not lrecIDs or (lrecIDs and lrecIDs.contains(j)):
                recdict[j] = recdict.get(j,0) + int((1 + math.log(tf[0])) * Gi * tf[1] * qtf)
                lrecIDs_remove[j] = 1 
                rec_termcount[j] = rec_termcount.get(j,0) + 1
        #Multiply with the number of terms of the total number of terms in the query existing in the records 
    else:
        #Use all records
        if qtf >= 0 or (qtf < 0 and len(recdict) == 0):
            for (j, tf) in invidx.iteritems():
                recdict[j] = recdict.get(j,0) + int((1 + math.log(tf[0])) * Gi * tf[1] * qtf)
                rec_termcount[j] = rec_termcount.get(j,0) + 1
        else: #much used term, do not include all records, only use already existing ones
            for (j, tf) in recdict.iteritems():
                if invidx.has_key(j):
                    tf = invidx[j]
                    recdict[j] = recdict.get(j,0) + int((1 + math.log(tf[0])) * Gi * tf[1] * qtf)
                    rec_termcount[j] = rec_termcount.get(j,0) + 1
        
    if verbose > 0:
        print "Calculation time: %s,%s" % (str(time.time() - startCreate), term) 
    return (recdict, rec_termcount, lrecIDs_remove)

def post_calculate_record_relevance(recdict, rec_termcount, lrecIDs_remove, lrecIDs, verbose):
    """Calculating the relevance of the documents based on the input"""

    startCreate = time.time()

    if lrecIDs:
        #Multiply with the number of terms of the total number of terms in the query existing in the records 
        for j in lrecIDs_remove.keys():
            lrecIDs.remove(j)
            recdict[j] = recdict[j] * rec_termcount[j]   
    else:
        #Multiply with the number of terms of the total number of terms in the query existing in the records
        for (j, tf) in rec_termcount.iteritems():
            recdict[j] = recdict[j] * rec_termcount[j]

    if verbose > 0:
        print "Post Calculation time: %s" % (str(time.time() - startCreate)) 
    return recdict

def sort_record_relevance(recdict, rank_limit_relevance,recID, verbose):
    """Sorts the dictionary and returns records with a relevance higher than the given value."""

    startCreate = time.time()
    divideby = max(recdict.values())

    reclist = []
    for (recid, w) in recdict.iteritems():
	w = int((w * 100 / divideby))
	if w >= rank_limit_relevance:
            reclist.append((recid,w))
    reclist.sort(lambda x, y: cmp(x[1], y[1]))

    #for (recid, w) in recdict.iteritems():
    if verbose > 0:
        print "Sort time: %s" % (str(time.time() - startCreate))
        print "Number of records sorted: %s" % len(reclist)

    return reclist

def stat(reclist, lwords):
    """Shows some statistics about the searchresult."""

    if len(reclist) > 20:
	j = 20
    else:
	j = len(reclist)

    for i in range(1, j):
   	print i,reclist[len(reclist) - i]
	res = run_sql("SELECT termlist FROM %sR WHERE id_bibrec=%s" % (rnkWORD_table[:-1],reclist[len(reclist) - i][0]))
	try:
	    termlist = deserialize_via_marshal(res[0][0])
            for term in lwords:
                if termlist.has_key(term):
                    print term, termlist[term]
	except StandardError, e:
	    pass

    count = {}
    for i in range(0, len(reclist)):
        count[reclist[i][1]] = count.get(reclist[i][1], 0) + 1
    i = 100
    while i >= 0:
        if count.has_key(i):
            print "%s-%s" % (i,count[i])
        i -= 1

try:
    import psyco
    psyco.bind(find_similar) 
    psyco.bind(rank_by_method)
    psyco.bind(calculate_record_relevance)
    psyco.bind(post_calculate_record_relevance)
    psyco.bind(word_frequency)
    psyco.bind(sort_record_relevance)
    psyco.bind(serialize_via_numeric_array)
    psyco.bind(serialize_via_marshal)
    psyco.bind(deserialize_via_numeric_array)
    psyco.bind(deserialize_via_marshal)
except StandardError, e:
    print "Psyco Error",e

