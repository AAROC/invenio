##Ranking of records using different parameters and methods on the fly.

## This file is part of the CERN Document Server Software (CDSware).
## Copyright (C) 2002 CERN.
##
## The CDSware is free software; you can redistribute it and/or
## modify it under the terms of the GNU General Public License as
## published by the Free Software Foundation; either version 2 of the
## License, or (at your option) any later version.
##
## The CDSware is distributed in the hope that it will be useful, but
## WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
## General Public License for more details.
##
## You should have received a copy of the GNU General Public License
## along with CDSware; if not, write to the Free Software Foundation, Inc.,
## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

## read config variables:
#include "config.wml"
#include "configbis.wml"
#include "cdswmllib.wml"

## start Python:
<protect>#!</protect><PYTHON>
<protect># -*- coding: utf-8 -*-</protect>
<protect>## $Id$</protect>
<protect>## DO NOT EDIT THIS FILE!  IT WAS AUTOMATICALLY GENERATED FROM CDSware WML SOURCES.</protect>
## fill config variables:
pylibdir = "<LIBDIR>/python"

try:
    import zlib
    import marshal
    import string
    import sys
    import time
    import math
    import MySQLdb
    import Numeric
    import re
    import ConfigParser
    import traceback
    import copy
except ImportError, e:
    import sys

try:
    import Stemmer
except ImportError, e:
    pass

try:
    sys.path.append('%s' % pylibdir)
    from cdsware.config import *
    from cdsware.dbquery import run_sql
    from search_engine_config import *

except ImportError, e:
    import sys

voutput = ""

class HitSet:
    """Class describing set of records, implemented as bit vectors of recIDs.
    Using Numeric arrays for speed (1 value = 8 bits), can use later "real"
    bit vectors to save space."""

    def __init__(self, init_set=None):
        self._nbhits = -1
        if init_set:
            self._set = init_set
        else:
            self._set = Numeric.zeros(cfg_max_recID+1, Numeric.Int0)

    def __repr__(self, join=string.join):
        return "%s(%s)" % (self.__class__.__name__, join(map(repr, self._set), ', '))

    def add(self, recID):
        "Adds a record to the set."
        self._set[recID] = 1

    def addmany(self, recIDs):
        "Adds several recIDs to the set."
        for recID in recIDs: self._set[recID] = 1

    def addlist(self, arr):
        "Adds an array of recIDs to the set."
        Numeric.put(self._set, arr, 1)

    def remove(self, recID):
        "Removes a record from the set."
        self._set[recID] = 0

    def removemany(self, recIDs):
        "Removes several records from the set."
        for recID in recIDs:
            self.remove(recID)

    def intersect(self, other):
        "Does a set intersection with other.  Keep result in self."
        self._set = Numeric.bitwise_and(self._set, other._set)

    def union(self, other):
        "Does a set union with other. Keep result in self."
        self._set = Numeric.bitwise_or(self._set, other._set)

    def difference(self, other):
        "Does a set difference with other. Keep result in self."
        #self._set = Numeric.bitwise_not(self._set, other._set)
        for recID in Numeric.nonzero(other._set):
            self.remove(recID)

    def contains(self, recID):
        "Checks whether the set contains recID."
        return self._set[recID]

    __contains__ = contains     # Higher performance member-test for python 2.0 and above

    def __getitem__(self, index):
        "Support for the 'for item in set:' protocol."
        return Numeric.nonzero(self._set)[index]
        
    def calculate_nbhits(self):
        "Calculates the number of records set in the hitset."
        self._nbhits = Numeric.sum(self._set.copy().astype(Numeric.Int))

    def items(self):
        "Return an array containing all recID."
        return Numeric.nonzero(self._set)

    def tolist(self):
        "Return an array containing all recID."
        return Numeric.nonzero(self._set).tolist()

def compare_on_val(first, second):
    return cmp(second[1], first[1])
def serialize_via_numeric_array_dumps(arr):
    return Numeric.dumps(arr)
def serialize_via_numeric_array_compr(str):
    return zlib.compress(str)
def serialize_via_numeric_array_escape(str):
    return MySQLdb.escape_string(str)
def serialize_via_numeric_array(arr):
    """Serialize Numeric array into a compressed string."""
    return serialize_via_numeric_array_escape(serialize_via_numeric_array_compr(serialize_via_numeric_array_dumps(arr)))
def deserialize_via_numeric_array(string):
    """Decompress and deserialize string into a Numeric array."""
    return Numeric.loads(zlib.decompress(string))
def serialize_via_marshal(obj):
    """Serialize Python object via marshal into a compressed string."""
    return MySQLdb.escape_string(zlib.compress(marshal.dumps(obj)))
def deserialize_via_marshal(string):
    """Decompress and deserialize string into a Python object via marshal."""
    return marshal.loads(zlib.decompress(string))

def adderrorbox(header='', datalist=[]):
    """used to create table around main data on a page, row based"""

    try: perc= str(100 // len(datalist)) + '%'
    except ZeroDivisionError: perc= 1

    output  = '<table class="errorbox">'
    output += '<thead><tr><th class="errorboxheader" colspan="%s">%s</th></tr></thead>' % (len(datalist), header)
    output += '<tbody>'
    for row in [datalist]:
        output += '<tr>'
        for data in row:
            output += '<td style="vertical-align: top; margin-top: 5px; width: %s;">' % (perc, )
            output += data
            output += '</td>'
        output += '</tr>'
    output += '</tbody></table>'
    return output

def check_term(stopwords, term, col_size, term_rec, max_occ, min_occ, termlength):
    """Check if the term is valid for use
    stopwords - a dict of stopwords
    term - the term to check
    col_size - the number of records in database
    term_rec - the number of records which contains this term
    max_occ - max frequency of the term allowed
    min_occ - min frequence of the term allowed
    termlength - the minimum length of the terms allowed"""
    try:
        if stopwords.has_key(term) or (len(term) <= termlength) or ((float(term_rec) / float(col_size)) >= max_occ) or ((float(term_rec) / float(col_size)) <= min_occ):
	    return ""
        if int(term):
            return ""
    except StandardError, e:
	pass
    return "true"

def get_stopwords(file='stopwords.kb'):
    """Load the stopwords from the file and returns a dict cotaining the words."""
    stopwords = open(file, 'r')
    lines = stopwords.readlines()
    stopwords.close()
    stopwords = {}
    for line in lines:
        stopwords[string.rstrip(line)] = 1
    return stopwords

def get_config(rank_method_code): #needs some work
    """Load common data into memory"""
    global voutput
    global stopwords
    global stemmer
    global chars_alphanumericseparators
    global col_size
    global rnkWORD_table
    languages = {'fr': 'french', 'en': 'porter', 'no':'norwegian', 'se':'swedish', 'de': 'german', 'it':'italian', 'pt':'portugese'}
    voutput = ""

    try: 
        if stemmer and stopwords:     
            pass
    except StandardError, e:
        if methods[rank_method_code].has_key("rnkWORD_table"):
            rnkWORD_table = methods[rank_method_code]["rnkWORD_table"]
        try:
            if methods[rank_method_code].has_key("stem_lang"):
                stemmer = Stemmer.Stemmer(languages[methods[rank_method_code]["stem_lang"]])
            else: 
                stemmer = None         
        except Exception, e:
            stemmer = None
        if methods[rank_method_code].has_key("stopword"):
            stopwords = get_stopwords("%s" % methods[rank_method_code]["stopword"]) 
        else:
            stopwords = {}
        chars_alphanumericseparators = r"[1234567890\!\"\#\$\%\&\'\(\)\*\+\,\-\.\/\:\;\<\=\>\?\@\[\\\]\^\_\`\{\|\}\~]"
        if methods[rank_method_code].has_key("rnkWORD_table"):
            col_size = run_sql("SELECT count(*) FROM %sR" % rnkWORD_table[:-1])[0][0]

def create_rnkmethod_cache():
    """Create cache with vital information for each rank method."""

    global methods
    bibrank_meths = run_sql("SELECT name from rnkMETHOD")
    methods = {}
    for (rank_method_code,) in bibrank_meths:
        try:
            file = etcdir + "/bibrank/" + rank_method_code + ".cfg"
            config = ConfigParser.ConfigParser()
            config.readfp(open(file))
        except StandardError, e:
            pass
        cfg_function = config.get("rank_method", "function")
        methods[rank_method_code] = {}
        methods[rank_method_code]["function"] = cfg_function
        methods[rank_method_code]["prefix"] = config.get(cfg_function, "prefix")
        methods[rank_method_code]["postfix"] = config.get(cfg_function, "postfix")

        if config.has_option(cfg_function, "table"):
            methods[rank_method_code]["rnkWORD_table"] = config.get(cfg_function, "table")
        if  config.has_option(cfg_function, "stem_if_avail") and config.get(cfg_function, "stem_if_avail") == "yes":
            methods[rank_method_code]["stem_lang"] = config.get(cfg_function, "stem_query_language")
        if config.has_option(cfg_function, "stopword"):
            methods[rank_method_code]["stopword"] = config.get(cfg_function, "stopword")
        if config.has_section("find_similar"):
            methods[rank_method_code]["max_word_occurence"] = float(config.get("find_similar", "max_word_occurence"))
            methods[rank_method_code]["min_word_occurence"] = float(config.get("find_similar", "min_word_occurence"))
            methods[rank_method_code]["min_word_length"] = int(config.get("find_similar", "min_word_length"))
            methods[rank_method_code]["min_nr_words_docs"] = int(config.get("find_similar", "min_nr_words_docs"))
            methods[rank_method_code]["max_nr_words_upper"] = int(config.get("find_similar", "max_nr_words_upper"))
            methods[rank_method_code]["max_nr_words_lower"] = int(config.get("find_similar", "max_nr_words_lower"))
            methods[rank_method_code]["override_default_min_relevance"] = config.get("find_similar", "override_default_min_relevance")
            methods[rank_method_code]["default_min_relevance"] = int(config.get("find_similar", "default_min_relevance"))

        i8n_names = run_sql("SELECT ln,value from rnkMETHODNAME,rnkMETHOD where id_rnkMETHOD=rnkMETHOD.id and rnkMETHOD.name='%s'" % (rank_method_code))
        for (ln, value) in i8n_names:
            methods[rank_method_code][ln] = value
            
def is_method_valid(colID, rank_method_code):
    """Checks if a method is valid for the collection given"""
    
    enabled_colls = dict(run_sql("SELECT id_collection, score from collection_rnkMETHOD,rnkMETHOD WHERE id_rnkMETHOD=rnkMETHOD.id AND name='%s'" % rank_method_code))

    colID = int(colID)
    if enabled_colls.has_key(colID):
        return 1
    else:
        while colID:
            colID = run_sql("SELECT id_dad FROM collection_collection WHERE id_son=%s" % colID)
            if colID and enabled_colls.has_key(colID[0][0]):
                return 1
            elif colID:
                colID = colID[0][0]
    return 0

def get_bibrank_methods(collection,ln=cdslang):
    """Returns a list of rank methods and the name om them in the language defined by the ln parameter, if collection is given, only methods enabled for that collection is returned."""

    try:
        if methods:
            pass
    except Exception:
        create_rnkmethod_cache()

    avail_methods = []
    for (rank_method_code, options) in methods.iteritems():
        if options.has_key("function") and is_method_valid(collection, rank_method_code):
            if options.has_key(ln):
                avail_methods.append((rank_method_code, options[ln]))
            else:
                avail_methods.append((rank_method_code, "Not translated"))              
    return avail_methods
    
def rank_records(rank_method_code, rank_limit_relevance, hitset_global, pattern=[], verbose=0):
    """rank_method, e.g. `jif' or `sbr' (word frequency vector model)                    
       rank_limit_relevance, e.g. `23' for `nbc' (number of citations) or `0.10' for `vec'                   
       hitset, search engine hits; optional                   
       pattern, search engine query or record ID (you check the type)                   
       verbose, verbose level
    """

    hitset = copy.deepcopy(hitset_global) #we are receiving a global hitset

    try:
        if methods:
            pass
    except Exception:
        create_rnkmethod_cache()

    try:
        function = methods[rank_method_code]["function"]
        func_object = globals().get(function)
        get_config(rank_method_code)

        if func_object and pattern and pattern[0][0:6] == "recid:":
            result = find_similar(rank_method_code, pattern[0][6:], hitset, rank_limit_relevance, verbose)
        elif func_object:
            result = func_object(rank_method_code, pattern, hitset, rank_limit_relevance, verbose)
        else:
            result = rank_by_method(rank_method_code, pattern, hitset, rank_limit_relevance, verbose)
    except Exception, e:
        result = (None, "", adderrorbox("An error occured when trying to rank the search result", ["Unexpected error: %s<br><b>Traceback:</b>%s" % (e, format_output(traceback.format_tb(sys.exc_info()[2])))]), voutput)

    if result[0]:
        results_similar_recIDs = map(lambda x: x[0], result[0])
        results_similar_relevances = map(lambda x: x[1], result[0])
        result = (results_similar_recIDs, results_similar_relevances, result[1], result[2], result[3])
    else:
        result = (None, None, result[1], result[2], result[3])

    if verbose > 0:
        print string.replace(voutput, "<br>", "\n")
    return result

def find_similar(rank_method_code, recID, hitset, rank_limit_relevance,verbose):
    """Finding terms to use for calculating similarity. Terms are taken from the recid given, returns a list of recids's and relevance, [[23,34], [344,24], [1,01]]
    recID - record to use for find similar
    rank_limit_relevance - find all similar document above given percentage (0-100)
    verbose - how much debug information to show, 0-9"""

    startCreate = time.time()
    global voutput

    if methods[rank_method_code]["override_default_min_relevance"] == "no":
        rank_limit_relevance = methods[rank_method_code]["default_min_relevance"]
   
    query_terms = {} 
    recID = int(recID)

    res = run_sql("SELECT termlist FROM %sR WHERE id_bibrec=%s" % (rnkWORD_table[:-1], recID))
    if not res:
        return (None, "Warning: Requested record does not seem to exist.", "", voutput) 
    rec_terms = deserialize_via_marshal(res[0][0])

    #Get all documents using terms from the selected documents
    if len(rec_terms) > 0:
        terms = "%s" % dict(rec_terms).keys()
        terms = terms[1:len(terms) - 1]
        terms_recs = dict(run_sql("SELECT term, hitlist FROM %s WHERE term IN (%s)" % (rnkWORD_table,terms)))
    else:
        return (None, "Warning, An error has occured (3)", "", voutput)


    #Calculate all terms
    for (term, tf) in rec_terms.iteritems():
	if len(term) >= methods[rank_method_code]["min_word_length"] and terms_recs.has_key(term):
            query_terms[term] =  int((1 + math.log(tf[0])) *  tf[1])
  
    query_terms_old = query_terms.items()
    query_terms_old.sort(lambda x, y: cmp(y[1], x[1])) 
    query_terms = {}

    stime = time.time()
    (recdict, rec_termcount, lrecIDs_remove) = ({}, {}, {})

    #Use only most important terms
    for (t, tf) in query_terms_old:
        term_recs = deserialize_via_marshal(terms_recs[t])
        if len(query_terms_old) <= methods[rank_method_code]["max_nr_words_lower"] or (len(term_recs) >= methods[rank_method_code]["min_nr_words_docs"] and (((float(len(term_recs)) / float(col_size)) <=  methods[rank_method_code]["max_word_occurence"]) and ((float(len(term_recs)) / float(col_size)) >= methods[rank_method_code]["min_word_occurence"]))):
             query_terms[t] = round(tf, 4) 
             (recdict, rec_termcount, lrecIDs_remove) = calculate_record_relevance((t, query_terms[t]) , term_recs, hitset, recdict, rec_termcount, lrecIDs_remove, verbose, "yes") 
             if verbose > 0:
                 voutput += "Term: %s,Number of records: %s, %s<br>" % (t, len(recdict), tf)

        if len(query_terms_old) > methods[rank_method_code]["max_nr_words_lower"] and (len(query_terms) ==  methods[rank_method_code]["max_nr_words_upper"] or tf < 0):
            break

    if len(query_terms) == 0: #not enough terms to get a good result
        return (None, "Warning, An error has occured (4)", "", voutput)

    if verbose > 0:
        voutput += "<br>Number of terms: %s<br>" % run_sql("SELECT count(id) FROM %s" % rnkWORD_table)[0][0]
        voutput += "Number of terms to use for query: %s<br>" % (len(query_terms))
        voutput += "Current number of recIDs: %s<br>" % (col_size)
        voutput += "Prepare time: %s<br>" % (str(time.time() - startCreate))

    recdict = post_calculate_record_relevance(recdict, rec_termcount, lrecIDs_remove, hitset, verbose)
    reclist = sort_record_relevance(recdict, rank_limit_relevance,recID, verbose)

    if verbose > 0:
        voutput += "Total time used: %s<br>" % (str(time.time() - startCreate))
        rank_method_stat(reclist, query_terms)

    return (reclist[:len(reclist)], methods[rank_method_code]["prefix"], methods[rank_method_code]["postfix"], voutput)

def rank_by_method(rank_method_code, lwords, hitset, rank_limit_relevance,verbose):
    """input: list of words, ['ellis', 'muon']          
    optional list of recIDs
    output: sorted list of recIDs based on rank method given, e.g. [[23,34], [344,24], [1,01]]           
    if not possible, then return empty list""" 

    global voutput
    rnkdict = run_sql("SELECT relevance_data FROM rnkMETHODDATA,rnkMETHOD where rnkMETHOD.id=id_rnkMETHOD and rnkMETHOD.name='%s'" % rank_method_code)
    if not rnkdict:
        return (None, "Warning, An error has occured (5)", "", voutput)
 
    rnkdict = deserialize_via_marshal(rnkdict[0][0])
    lrecIDs = hitset.items()
    reclist = []
    reclist_addend = []

    for recID in lrecIDs:
        if rnkdict.has_key(recID):
            reclist.append((recID, rnkdict[recID][1]))
            del rnkdict[recID]
        else:
            reclist_addend.append((recID, 0))

    reclist.sort(lambda x, y: cmp(x[1], y[1]))
    return (reclist_addend + reclist, methods[rank_method_code]["prefix"], methods[rank_method_code]["postfix"], "")

def word_frequency(rank_method_code, lwords, hitset, rank_limit_relevance,verbose):
    """input: list of words, ['ellis', 'muon']          
    optional list of recIDs
    output: sorted list of recIDs by summary word frequencies, e.g. [[23,34], [344,24], [1,01]]           
    if not possible (e.g. all stopwords), then return empty list""" 

    global voutput
    startCreate = time.time()
    query_terms = {}
    lwords_old = lwords
    lwords = []
 
    #Check terms, remove non alphanumeric characters. Use both unstemmed and stemmed version of all terms.
    for i in range(0, len(lwords_old)):
        term = string.lower(lwords_old[i])
        use_term = 0
        if not stopwords.has_key(term):
            lwords.append(term)
            terms = string.split(string.lower(re.sub(chars_alphanumericseparators, ' ', term)))  
            for term in terms: 
                if stemmer: # stem word
                    term = stemmer.stem(string.replace(term, ' ', ''))
                if lwords_old[i] != term: #add if stemmed word is different than original word
	            lwords.append(term)

    (recdict, rec_termcount, lrecIDs_remove) = ({}, {}, {})
    #For each term, if accepted, get a list of the records using the term
    #calculate then relevance for each term before sorting the list of records
    for term in lwords:
	term_recs = run_sql("SELECT term, hitlist FROM %s WHERE term='%s'" % (rnkWORD_table,  MySQLdb.escape_string(term)))
        if term_recs:
	    term_recs = deserialize_via_marshal(term_recs[0][1])
            if check_term({}, term, col_size, len(term_recs), 1.0, 0.00, 0):
                query_terms[term] = int(query_terms.get(term, 0) + term_recs["Gi"][1])
                (recdict, rec_termcount, lrecIDs_remove) = calculate_record_relevance((term, query_terms[term]) , term_recs, hitset, recdict, rec_termcount, lrecIDs_remove, verbose)
            del term_recs

    if len(recdict) == 0 or (len(lwords) == 1 and lwords[0] == ""):
        return (None, "Records not ranked. The query is not detailed enough for ranking to be possible.", "", voutput)

    if verbose > 0:
        voutput += "<br>Current number of recIDs: %s<br>" % (col_size)
        voutput += "Number of terms: %s<br>" % run_sql("SELECT count(id) FROM %s" % rnkWORD_table)[0][0]
        voutput += "Terms: %s<br>" % query_terms
        voutput += "Prepare and pre calculate time: %s<br>" % (str(time.time() - startCreate))

    recdict = post_calculate_record_relevance(recdict, rec_termcount, lrecIDs_remove, hitset, verbose)
    reclist = sort_record_relevance(recdict, rank_limit_relevance, 0, verbose)

    #Add any documents not ranked to the end of the list
    if hitset:
        hitset.calculate_nbhits()
    if hitset and len(reclist) < hitset._nbhits: #add records found but not in list
        lrecIDs = hitset.tolist()                                #using 2-3mb 
        reclist = zip(lrecIDs, [0] * len(lrecIDs)) + reclist      #using 6mb

    if verbose > 0:
        voutput += "Total time used: %s<br>" % (str(time.time() - startCreate))
        rank_method_stat(reclist, query_terms)

    return (reclist, methods[rank_method_code]["prefix"], methods[rank_method_code]["postfix"], voutput)

def calculate_record_relevance(term, invidx, hitset, recdict, rec_termcount, lrecIDs_remove, verbose, quick=None):
    """Calculating the relevance of the documents based on the input"""

    (t, qtf) = term
    Gi = invidx["Gi"][1]
    del invidx["Gi"]
    if not quick or (qtf >= 0 or (qtf < 0 and len(recdict) == 0)):
        #Only accept records existing in the hitset received from the search engine
        for (j, tf) in invidx.iteritems():
            if hitset.contains(j):
                recdict[j] = recdict.get(j,0) + int((1 + math.log(tf[0])) * Gi * tf[1] * qtf)
                lrecIDs_remove[j] = 1 
                rec_termcount[j] = rec_termcount.get(j,0) + 1
        #Multiply with the number of terms of the total number of terms in the query existing in the records 
    elif quick: #much used term, do not include all records, only use already existing ones
        for (j, tf) in recdict.iteritems():
            if invidx.has_key(j):
                tf = invidx[j]
                recdict[j] = recdict[j] + int((1 + math.log(tf[0])) * Gi * tf[1] * qtf)
                rec_termcount[j] = rec_termcount.get(j,0) + 1
        
    return (recdict, rec_termcount, lrecIDs_remove)

def post_calculate_record_relevance(recdict, rec_termcount, lrecIDs_remove, hitset, verbose):
    """Calculating the relevance of the documents based on the input"""

    startCreate = time.time()
    global voutput

    #Multiply with the number of terms of the total number of terms in the query existing in the records 
    for j in lrecIDs_remove.keys():
        hitset.remove(j)
        recdict[j] = math.log(recdict[j] * rec_termcount[j])

    if verbose > 0:
        voutput += "Post Calculation time: %s<br>" % (str(time.time() - startCreate)) 
    return recdict

def sort_record_relevance(recdict, rank_limit_relevance,recID, verbose):
    """Sorts the dictionary and returns records with a relevance higher than the given value."""

    startCreate = time.time()
    global voutput
    reclist = []
    divideby = max(recdict.values())

    for (recid, w) in recdict.iteritems():
	w = int(w * 100 / divideby)
	if w >= rank_limit_relevance:
            reclist.append((recid, w))
    reclist.sort(lambda x, y: cmp(x[1], y[1]))

    if verbose > 0:
        voutput += "Number of records sorted: %s<br>" % len(reclist)
        voutput += "Sort time: %s<br>" % (str(time.time() - startCreate))

    return reclist

def rank_method_stat(reclist, lwords):
    """Shows some statistics about the searchresult."""
    global voutput
    if len(reclist) > 20:
	j = 20
    else:
	j = len(reclist)

    voutput += "<br>Rank statistics:<br>"
    for i in range(1, j):
   	voutput += "%s,Recid:%s,Score:%s<br>" % (i,reclist[len(reclist) - i][0],reclist[len(reclist) - i][1])
	res = run_sql("SELECT termlist FROM %sR WHERE id_bibrec=%s" % (rnkWORD_table[:-1],reclist[len(reclist) - i][0]))
	try:
	    termlist = deserialize_via_marshal(res[0][0])
            for term in lwords:
                if termlist.has_key(term):
                    voutput += "%s-%s / " % (term, termlist[term][0])
            voutput += "<br>"
	except StandardError, e:
	    pass

    voutput += "<br>Score variation:<br>"
    count = {}
    for i in range(0, len(reclist)):
        count[reclist[i][1]] = count.get(reclist[i][1], 0) + 1
    i = 100
    while i >= 0:
        if count.has_key(i):
            voutput += "%s-%s<br>" % (i,count[i])
        i -= 1


try:
    import psyco
    psyco.bind(find_similar) 
    psyco.bind(rank_by_method)
    psyco.bind(calculate_record_relevance)
    psyco.bind(post_calculate_record_relevance)
    psyco.bind(word_frequency)
    psyco.bind(sort_record_relevance)
    psyco.bind(serialize_via_numeric_array)
    psyco.bind(serialize_via_marshal)
    psyco.bind(deserialize_via_numeric_array)
    psyco.bind(deserialize_via_marshal)
except StandardError, e:
    print "Psyco Error",e

