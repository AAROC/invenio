## $Id$
## CDSware Search Engine in mod_python.

## This file is part of the CERN Document Server Software (CDSware).
## Copyright (C) 2002 CERN.
##
## The CDSware is free software; you can redistribute it and/or
## modify it under the terms of the GNU General Public License as
## published by the Free Software Foundation; either version 2 of the
## License, or (at your option) any later version.
##
## The CDSware is distributed in the hope that it will be useful, but
## WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
## General Public License for more details.  
##
## You should have received a copy of the GNU General Public License
## along with CDSware; if not, write to the Free Software Foundation, Inc.,
## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.

## read config variables:
#include "config.wml"
#include "configbis.wml"

## start Python:
<protect>## $Id$</protect>
<protect>## DO NOT EDIT THIS FILE! IT WAS AUTOMATICALLY GENERATED FROM CDSware WML SOURCES.</protect>
"""CDSware Search Engine Callable Interface."""

## fill config variables:
pylibdir = "<LIBDIR>/python"

try:
    import sys
    sys.path.append('%s' % pylibdir)
    from cdsware.search_engine import *
except ImportError, e:
    print "Error: %s" % e
    import sys
    sys.exit(1)

<protect> 
__version__ = "$Id$"

def search(req, cc=cdsname, c=None, p="", f="", rg="10", sf="", so="d", sp="", of="hb", ot="", as="0",
           p1="", f1="", m1="", op1="", p2="", f2="", m2="", op2="", p3="", f3="", m3="", sc="0", jrec="0",
           id="-1", idb="-1", sysnb="", search="SEARCH"):
    """Fallback entry point to WebSearch, for backward compatibility."""
    index(req, cc, c, p, f, rg, sf, so, sp, of, ot, as,
              p1, f1, m1, op1, p2, f2, m2, op2, p3, f3, m3, sc, jrec,
              id, idb, sysnb, search)
    return "\n"
    
def index(req, cc=cdsname, c=None, p="", f="", rg="10", sf="", so="d", sp="", of="hb", ot="", as="0",
              p1="", f1="", m1="", op1="", p2="", f2="", m2="", op2="", p3="", f3="", m3="", sc="0", jrec="0",
              id="-1", idb="-1", sysnb="", search="SEARCH"):
    """Main entry point to WebSearch."""
    # wash passed numerical arguments:
    try:
        id = string.atoi(id)
        idb = string.atoi(idb)
    except:
        id = cfg_max_recID + 1
        idb = cfg_max_recID + 1
    sc = string.atoi(sc)
    jrec = string.atoi(jrec)
    rg = string.atoi(rg)
    as = string.atoi(as)
    if type(of) is list:
        of = of[0]
    if type(ot) is list:
        ot = string.join(ot,",")
    if of.startswith('x'):
        # we are doing XML output:
        req.content_type = "text/xml"
        req.send_http_header()
        req.write("""<?xml version="1.0" encoding="UTF-8"?>\n""")
        if of.startswith("xm"):
            req.write("""<collection xmlns="http://www.loc.gov/MARC21/slim">\n""")
        else:
            req.write("""<collection>\n""")
    elif of.startswith('t') or str(of[0:3]).isdigit():
        # we are doing plain text output:
        req.content_type = "text/plain"
        req.send_http_header()
    else:
        # we are doing HTML output:
        req.content_type = "text/html"
        req.send_http_header()
        # write header:
        req.write(create_header(cc))
    if sysnb or id>0:
        ## 1 - detailed record display
        if sysnb: # ALEPH sysnb is passed, so deduce MySQL id for the record:            
            id = get_mysql_recid_from_aleph_sysno(sysnb)
        if of=="hb":
            of = "hd"
        if record_exists(id):
            if idb<=id: # sanity check
                idb=id+1
            print_records(req, range(id,idb), -1, -9999, of, ot)
        else: # record does not exist
            if of.startswith("h"):
                (cc, colls_to_display, colls_to_search) = wash_colls(cc, c, sc)
                p = wash_pattern(p)
                f = wash_field(f)
                req.write(create_search_box(cc, colls_to_display, p, f, rg, sf, so, sp, of, ot, as, p1, f1, m1, op1,
                                            p2, f2, m2, op2, p3, f3, m3, sc))
                print_warning(req, "Requested record does not seem to exist.", None, "<p>")
    elif search == "Browse":
        ## 2 - browse needed
        (cc, colls_to_display, colls_to_search) = wash_colls(cc, c, sc)
        p = wash_pattern(p)
        f = wash_field(f)
        # write search box:
        if of.startswith("h"):
            req.write(create_search_box(cc, colls_to_display, p, f, rg, sf, so, sp, of, ot, as, p1, f1, m1, op1,
                                        p2, f2, m2, op2, p3, f3, m3, sc))
        url = string.replace(req.args, "search=Browse","search=SEARCH")
        if as==1 or (p1 or p2 or p3):
            if p1:
                req.write("<p>Words nearest to <strong>%s</strong> inside <strong>%s</strong> are:<br>" % (p1, f1))
                req.write(create_nearest_words_links(url, p1, f1))
            if p2:
                req.write("<p>Words nearest to <strong>%s</strong> inside <strong>%s</strong> are:<br>" % (p2, f2))
                req.write(create_nearest_words_links(url, p2, f2))
            if p3:
                req.write("<p>Words nearest to <strong>%s</strong> inside <strong>%s</strong> are:<br>" % (p3, f3))
                req.write(create_nearest_words_links(url, p3, f3))
        else:
            req.write("<p>Words nearest to <strong>%s</strong> inside <strong>%s</strong> are:<br>" % (p, f))
            req.write(create_nearest_words_links(url, p, f))
    else:
        ## 3 - search needed
        # wash passed collection arguments:
        (cc, colls_to_display, colls_to_search) = wash_colls(cc, c, sc)
        p = wash_pattern(p)
        f = wash_field(f)
        # write search box:
        if of.startswith("h"):
            req.write(create_search_box(cc, colls_to_display, p, f, rg, sf, so, sp, of, ot, as, p1, f1, m1, op1,
                                        p2, f2, m2, op2, p3, f3, m3, sc))
        # run search:
        t1 = os.times()[4]
        if as == 1 or (p1 or p2 or p3):
            # 3A - advanced search
            results_final = search_pattern(req, "", "", colls_to_search)
            if p1:
                results_tmp = search_pattern(req, p1, f1, colls_to_search, m1)
                for coll in colls_to_search: # join results for first advanced search boxen
                    results_final[coll].intersect(results_tmp[coll])
            if p2:
                results_tmp = search_pattern(req, p2, f2, colls_to_search, m2)
                for coll in colls_to_search: # join results for first and second advanced search boxen
                    if op1 == "a": # add
                        results_final[coll].intersect(results_tmp[coll])
                    elif op1 == "o": # or
                        results_final[coll].union(results_tmp[coll])
                    elif op1 == "n": # not
                        results_final[coll].difference(results_tmp[coll])
                    else:
                        print_warning(req, "Invalid set operation %s." % op1, "Error")
            if p3:
                results_tmp = search_pattern(req, p3, f3, colls_to_search, m3)
                for coll in colls_to_search: # join results for second and third advanced search boxen
                    if op2 == "a": # add
                        results_final[coll].intersect(results_tmp[coll])
                    elif op2 == "o": # or
                        results_final[coll].union(results_tmp[coll])
                    elif op2 == "n": # not
                        results_final[coll].difference(results_tmp[coll])
                    else:
                        print_warning(req, "Invalid set operation %s." % op1, "Error")            
            for coll in colls_to_search:
                results_final[coll].calculate_nbhits()
        else:
            # 3B - simple search
            search_cache_key = p+"@"+f+"@"+string.join(colls_to_search,",")
            if search_cache.has_key(search_cache_key): # is the result in search cache?
                results_final = search_cache[search_cache_key]        
            else:       
                results_final = search_pattern(req, p, f, colls_to_search)
                search_cache[search_cache_key] = results_final
            if len(search_cache) > cfg_search_cache_size: # is the cache full? (sanity cleaning)
                search_cache.clear()
        t2 = os.times()[4]
        cpu_time = t2 - t1
        # find total number of records found in each collection
        results_final_nb_total = 0
        results_final_nb = {}
        for coll in colls_to_search:
            results_final_nb[coll] = results_final[coll]._nbhits
            results_final_nb_total += results_final_nb[coll]
        # was there at least one hit?
        if results_final_nb_total == 0:
            if of.startswith('h'):
                print_warning(req, "No match found.  Trying similar queries...", "", "<p>","<p>")
                req.write("<p>")
                if as==1 or (p1 or p2 or p3):
                    if p1:
                        search_pattern(req, p1, f1, colls_to_search, m1, 1)
                    if p2:
                        search_pattern(req, p2, f2, colls_to_search, m2, 1)
                    if p3:
                        search_pattern(req, p3, f3, colls_to_search, m3, 1)
                else:
                    search_pattern(req, p, f, colls_to_search, None, 1)
        else:
            # yes, some hits found, so print results overview:
            if of.startswith("h"):
                req.write(print_results_overview(colls_to_search, results_final_nb_total, results_final_nb, cpu_time))
            # print records:
            if len(colls_to_search)>1:
                cpu_time = -1 # we do not want to have search time printed on each collection
            for coll in colls_to_search:
                if results_final[coll]._nbhits:
                    if of.startswith("h"):
                        req.write(print_search_info(p, f, sf, so, sp, of, ot, coll, results_final_nb[coll], jrec, rg, as, p1, p2, p3, f1, f2, f3, m1, m2, m3, op1, op2, cpu_time))
                    results_final_sorted = results_final[coll].items()
                    if sf:
                        results_final_sorted = sort_records(req, results_final_sorted, sf, so, sp)
                    print_records(req, results_final_sorted, jrec, rg, of, ot)
                    if of.startswith("h"):
                        req.write(print_search_info(p, f, sf, so, sp, of, ot, coll, results_final_nb[coll], jrec, rg, as, p1, p2, p3, f1, f2, f3, m1, m2, m3, op1, op2, cpu_time, 1))
            # log query:
            log_query_info("ss", p, f, colls_to_search, results_final_nb_total)
    # 4 -- write footer:
    if of.startswith('h'):
        req.write(create_footer("http://"+req.hostname+req.uri))
    elif of.startswith('x'):
        req.write("""</collection>\n""")
    return "\n"

def cache(req, action="show"):
    """Manipulates the search engine cache."""
    global search_cache
    global collrecs_cache
    req.content_type = "text/html"
    req.send_http_header() 
    out = ""
    out += "<h1>Search Cache</h1>"
    # clear cache if requested:
    if action == "clear":
        search_cache = {}
        collrecs_cache = create_collrecs_cache()
        collrecs_cache[cdsname] = get_collection_hitlist(cdsname)
    # show collection cache:
    out += "<h3>Collection Cache</h3>"
    out += "<blockquote>"
    for coll in collrecs_cache.keys():
        if collrecs_cache[coll]:
            out += "%s<br>" % coll
    out += "</blockquote>"
    # show search cache:
    out += "<h3>Search Cache</h3>"
    out += "<blockquote>"
    if len(search_cache):
        out += """<table border="=">"""
        out += "<tr><td><strong>%s</strong></td><td><strong>%s</strong></td><td><strong>%s</strong></td><td><strong>%s</strong></td></tr>" % ("Pattern","Field","Collection","Number of Hits")
        for search_cache_key in search_cache.keys():
            p, f, c = string.split(search_cache_key, "@", 2)
            # find out about length of cached data:
            l = 0
            for coll in search_cache[search_cache_key]:
                l += search_cache[search_cache_key][coll]._nbhits
            out += "<tr><td>%s</td><td>%s</td><td>%s</td><td>%d</td></tr>" % (p, f, c, l)
        out += "</table>"
    else:
        out += "<p>Search cache is empty."
    out += "</blockquote>"
    out += """<p><a href="%s/search.py/cache?action=clear">clear cache</a>""" % weburl
    req.write(out)
    return "\n"

def log(req, date=""):
    """Display search log information for given date."""
    req.content_type = "text/html"
    req.send_http_header() 
    req.write("<h1>Search Log</h1>")
    if date: # case A: display stats for a day
        yyyymmdd = string.atoi(date)
        req.write("<p><big><strong>Date: %d</strong></big><p>" % yyyymmdd)
        req.write("""<table border="1">""")
        req.write("<tr><td><strong>%s</strong></td><td><strong>%s</strong></td><td><strong>%s</strong></td><td><strong>%s</strong></td><td><strong>%s</strong></td><td><strong>%s</strong></td></tr>" % ("No.","Time", "Pattern","Field","Collection","Number of Hits"))
        # read file:
        p = os.popen("grep ^%d %s/search.log" % (yyyymmdd,logdir), 'r')
        lines = p.readlines()
        p.close()
        # process lines:
        i = 0
        for line in lines:
            try:
                datetime, as, p, f, c, nbhits = string.split(line,"#")
                i += 1
                req.write("<tr><td align=\"right\">#%d</td><td>%s:%s:%s</td><td>%s</td><td>%s</td><td>%s</td><td>%s</td></tr>" \
                          % (i, datetime[8:10], datetime[10:12], datetime[12:], p, f, c, nbhits))
            except:
                pass # ignore eventual wrong log lines
        req.write("</table>")
    else: # case B: display summary stats per day
        yyyymm01 = int(time.strftime("%04Y%02m01", time.localtime()))
        yyyymmdd = int(time.strftime("%04Y%02m%02d", time.localtime()))
        req.write("""<table border="1">""")
        req.write("<tr><td><strong>%s</strong></td><td><strong>%s</strong></tr>" % ("Day", "Number of Queries"))
        for day in range(yyyymm01,yyyymmdd+1):
            p = os.popen("grep -c ^%d %s/search.log" % (day,logdir), 'r')
            for line in p.readlines():
                req.write("""<tr><td>%s</td><td align="right"><a href="%s/search.py/log?date=%d">%s</a></td></tr>""" % (day, weburl,day,line))
            p.close()
        req.write("</table>")
    return "\n"    

</protect>
