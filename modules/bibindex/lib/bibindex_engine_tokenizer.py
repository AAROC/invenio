# -*- coding:utf-8 -*-
##
## This file is part of Invenio.
## Copyright (C) 2010, 2011, 2012 CERN.
##
## Invenio is free software; you can redistribute it and/or
## modify it under the terms of the GNU General Public License as
## published by the Free Software Foundation; either version 2 of the
## License, or (at your option) any later version.
##
## Invenio is distributed in the hope that it will be useful, but
## WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
## General Public License for more details.
##
## You should have received a copy of the GNU General Public License
## along with Invenio; if not, write to the Free Software Foundation, Inc.,
## 59 Temple Place, Suite 330, Boston, MA 02111-1307, USA.
"""bibindex_engine_tokenizer: a set of classes implementing index tokenization

The idea is that Tokenizer classes provide a method, tokenize(), which turns
input strings into lists of strings.  The output strings are calculated based
on the input string as tokens suitable for word or phrase indexing.
"""

import os
import sys
import logging
import urllib2
import re


from invenio.config import \
     CFG_BIBINDEX_AUTHOR_WORD_INDEX_EXCLUDE_FIRST_NAMES, \
     CFG_CERN_SITE, \
     CFG_INSPIRE_SITE, \
     CFG_SOLR_URL, \
     CFG_XAPIAN_ENABLED, \
     CFG_BIBINDEX_FULLTEXT_INDEX_LOCAL_FILES_ONLY, \
     CFG_BIBINDEX_SPLASH_PAGES
from invenio.bibindex_engine_config import \
     CFG_BIBINDEX_WORDTABLE_TYPE
from invenio.htmlutils import remove_html_markup, \
                              get_links_in_html_page
from invenio.websubmit_file_converter import convert_file, get_file_converter_logger
from invenio.solrutils_bibindex_indexer import solr_add_fulltext
from invenio.xapianutils_bibindex_indexer import xapian_add
from invenio.textutils import wash_for_utf8, strip_accents
from invenio.bibdocfile import bibdocfile_url_p, \
     bibdocfile_url_to_bibdoc, download_url, \
     BibRecDocs
from invenio.bibindexadminlib import get_idx_indexer
from invenio.bibtask import write_message
from invenio.errorlib import register_exception
from invenio.bibindex_engine_washer import \
     lower_index_term, remove_latex_markup, \
     apply_stemming, remove_stopwords, length_check, \
     wash_author_name
from invenio.intbitset import intbitset
from invenio.dbquery import run_sql
from invenio.bibindex_engine_utils import latex_formula_re, \
     re_block_punctuation_begin, \
     re_block_punctuation_end, \
     re_punctuation, \
     re_separators, \
     re_arxiv, \
     get_field_count



if CFG_CERN_SITE:
    CFG_JOURNAL_TAG = '773__%'
    CFG_JOURNAL_PUBINFO_STANDARD_FORM = "773__p 773__v (773__y) 773__c"
    CFG_JOURNAL_PUBINFO_STANDARD_FORM_REGEXP_CHECK = r'^\w.*\s\w.*\s\(\d+\)\s\w.*$'
elif CFG_INSPIRE_SITE:
    CFG_JOURNAL_TAG = '773__%'
    CFG_JOURNAL_PUBINFO_STANDARD_FORM = "773__p,773__v,773__c"
    CFG_JOURNAL_PUBINFO_STANDARD_FORM_REGEXP_CHECK = r'^\w.*,\w.*,\w.*$'
else:
    CFG_JOURNAL_TAG = '909C4%'
    CFG_JOURNAL_PUBINFO_STANDARD_FORM = "909C4p 909C4v (909C4y) 909C4c"
    CFG_JOURNAL_PUBINFO_STANDARD_FORM_REGEXP_CHECK = r'^\w.*\s\w.*\s\(\d+\)\s\w.*$'




fulltext_added = intbitset() # stores ids of records whose fulltexts have been added


class BibIndexTokenizer(object):
    """Base class for the tokenizers

    Tokenizers act as filters which turn input strings into lists of strings
    which represent the idexable components of that string.
    """
    #words part
    def scan_string_for_words(self, s):
        """Return an intermediate representation of the tokens in s.

        Every tokenizer should have a scan_string function, which scans the
        input string and lexically tags its components.  These units are
        grouped together sequentially.  The output of scan_string is usually
        something like:
        {
            'TOKEN_TAG_LIST' : a list of valid keys in this output set,
            'key1' : [val1, val2, val3] - where key describes the in some
                      meaningful way
        }

        @param s: the input to be lexically tagged
        @type s: string

        @return: dict of lexically tagged input items
            In a sample Tokenizer where scan_string simply splits s on
            space, scan_string might output the following for
            "Assam and Darjeeling":
            {
                'TOKEN_TAG_LIST' : 'word_list',
                'word_list'     : ['Assam', 'and', 'Darjeeling']
            }
        @rtype: dict
        """
        raise NotImplementedError

    def parse_scanned_for_words(self, o):
        """Calculate the token list from the intermediate representation o.

        While this should be an interesting computation over the intermediate
        representation generated by scan_string, obviously in the split-on-
        space example we need only return o['word_list'].

        @param t: a dictionary with a 'word_list' key
        @type t: dict

        @return: the token items from 'word_list'
        @rtype: list of string
        """
        raise NotImplementedError

    def tokenize_for_words(self, s):
        """Main entry point.  Return token list from input string s.

        Simply composes the functionality above.

        @param s: the input to be lexically tagged
        @type s: string

        @return: the token items derived from s
        @rtype: list of string
        """
        raise NotImplementedError

    #pairs part
    def scan_string_for_pairs(self, s):
        """ See: scan_string_for_words """
        raise NotImplementedError

    def parse_scanned_for_pairs(self, o):
        """ See: parse_scanned_for_words """
        raise NotImplementedError

    def tokenize_for_pairs(self, s):
        """ See: tokenize_for_words """
        raise NotImplementedError

    #phrases part
    def scan_string_for_phrases(self, s):
        """ See: scan_string_for_words """
        raise NotImplementedError

    def parse_scanned_for_phrases(self, o):
        """ See: parse_scanned_for_words """
        raise NotImplementedError

    def tokenize_for_phrases(self, s):
        """ See: tokenize_for_words """
        raise NotImplementedError


    def get_function_by_type(self, wordtable_type):
        """Chooses tokenize_for_words, tokenize_for_phrases or tokenize_for_pairs
           depending on type of tokenization we want to perform."""
        raise NotImplementedError


class BibIndexEmptyTokenizer(BibIndexTokenizer):
    """Empty tokenizer do nothing.
       It returns empty lists for tokenize_for_words, tokenize_for_pairs and tokenize_for_phrases methods.
    """

    def __init__(self, stemming_language = None, remove_stopwords = 'No', remove_html_markup = 'No', remove_latex_markup = 'No'):
        """@param stemming_language: dummy
           @param remove_stopwords: dummy
           @param remove_html_markup: dummy
           @param remove_latex_markup: dummy
        """
        pass


    def get_function_by_type(self, wordtable_type):
        """Picks correct tokenize_for_xxx function depending on type of tokenization (wordtable_type)"""
        if wordtable_type == CFG_BIBINDEX_WORDTABLE_TYPE["Words"]:
            return self.tokenize_for_words
        elif wordtable_type == CFG_BIBINDEX_WORDTABLE_TYPE["Pairs"]:
            return self.tokenize_for_pairs
        elif wordtable_type == CFG_BIBINDEX_WORDTABLE_TYPE["Phrases"]:
            return self.tokenize_for_phrases


    def tokenize_for_words(self, phrase):
        return []

    def tokenize_for_pairs(self, phrase):
        return []

    def tokenize_for_phrases(self, phrase):
        return []


class BibIndexDefaultTokenizer(BibIndexTokenizer):
    """
        It's a standard tokenizer. It is useful for most of the indexes.
        Its behaviour depends on stemming, remove stopwords, remove html markup and remove latex markup parameters.
    """

    def __init__(self, stemming_language = None, remove_stopwords = 'No', remove_html_markup = 'No', remove_latex_markup = 'No'):
        """initialization"""
        self.stemming_language = stemming_language
        self.remove_stopwords = remove_stopwords
        self.remove_html_markup = remove_html_markup
        self.remove_latex_markup = remove_latex_markup


    def get_function_by_type(self, wordtable_type):
        """Picks correct tokenize_for_xxx function depending on type of tokenization (wordtable_type)"""
        if wordtable_type == CFG_BIBINDEX_WORDTABLE_TYPE["Words"]:
            return self.tokenize_for_words
        elif wordtable_type == CFG_BIBINDEX_WORDTABLE_TYPE["Pairs"]:
            return self.tokenize_for_pairs
        elif wordtable_type == CFG_BIBINDEX_WORDTABLE_TYPE["Phrases"]:
            return self.tokenize_for_phrases



    def tokenize_for_words(self, phrase):
        """Return list of words found in PHRASE.  Note that the phrase is
           split into groups depending on the alphanumeric characters and
           punctuation characters definition present in the config file.
        """

        words = {}
        formulas = []
        if self.remove_html_markup == 'Yes' and phrase.find("</") > -1:
            phrase = remove_html_markup(phrase)
        if self.remove_latex_markup == 'Yes':
            formulas = latex_formula_re.findall(phrase)
            phrase = remove_latex_markup(phrase)
            phrase = latex_formula_re.sub(' ', phrase)
        phrase = wash_for_utf8(phrase)
        phrase = lower_index_term(phrase)
        # 1st split phrase into blocks according to whitespace
        for block in strip_accents(phrase).split():
            # 2nd remove leading/trailing punctuation and add block:
            block = re_block_punctuation_begin.sub("", block)
            block = re_block_punctuation_end.sub("", block)
            if block:
                stemmed_block = remove_stopwords(block, self.remove_stopwords)
                stemmed_block = length_check(stemmed_block)
                stemmed_block = apply_stemming(stemmed_block, self.stemming_language)
                if stemmed_block:
                    words[stemmed_block] = 1
                if re_arxiv.match(block):
                    # special case for blocks like `arXiv:1007.5048' where
                    # we would like to index the part after the colon
                    # regardless of dot or other punctuation characters:
                    words[block.split(':', 1)[1]] = 1
                # 3rd break each block into subblocks according to punctuation and add subblocks:
                for subblock in re_punctuation.split(block):
                    stemmed_subblock = remove_stopwords(subblock, self.remove_stopwords)
                    stemmed_subblock = length_check(stemmed_subblock)
                    stemmed_subblock = apply_stemming(stemmed_subblock, self.stemming_language)
                    if stemmed_subblock:
                        words[stemmed_subblock] = 1
                    # 4th break each subblock into alphanumeric groups and add groups:
                    for alphanumeric_group in re_separators.split(subblock):
                        stemmed_alphanumeric_group = remove_stopwords(alphanumeric_group, self.remove_stopwords)
                        stemmed_alphanumeric_group = length_check(stemmed_alphanumeric_group)
                        stemmed_alphanumeric_group = apply_stemming(stemmed_alphanumeric_group, self.stemming_language)
                        if stemmed_alphanumeric_group:
                            words[stemmed_alphanumeric_group] = 1
        for block in formulas:
            words[block] = 1
        return words.keys()


    def tokenize_for_pairs(self, phrase):
        """Return list of words found in PHRASE.  Note that the phrase is
           split into groups depending on the alphanumeric characters and
           punctuation characters definition present in the config file.
        """

        words = {}
        if self.remove_html_markup == 'Yes' and phrase.find("</") > -1:
            phrase = remove_html_markup(phrase)
        if self.remove_latex_markup == 'Yes':
            phrase = remove_latex_markup(phrase)
            phrase = latex_formula_re.sub(' ', phrase)
        phrase = wash_for_utf8(phrase)
        phrase = lower_index_term(phrase)
        # 1st split phrase into blocks according to whitespace
        last_word = ''
        for block in strip_accents(phrase).split():
            # 2nd remove leading/trailing punctuation and add block:
            block = re_block_punctuation_begin.sub("", block)
            block = re_block_punctuation_end.sub("", block)
            if block:
                block = remove_stopwords(block, self.remove_stopwords)
                block = length_check(block)
                block = apply_stemming(block, self.stemming_language)
                # 3rd break each block into subblocks according to punctuation and add subblocks:
                for subblock in re_punctuation.split(block):
                    subblock = remove_stopwords(subblock, self.remove_stopwords)
                    subblock = length_check(subblock)
                    subblock = apply_stemming(subblock, self.stemming_language)
                    if subblock:
                        # 4th break each subblock into alphanumeric groups and add groups:
                        for alphanumeric_group in re_separators.split(subblock):
                            alphanumeric_group = remove_stopwords(alphanumeric_group, self.remove_stopwords)
                            alphanumeric_group = length_check(alphanumeric_group)
                            alphanumeric_group = apply_stemming(alphanumeric_group, self.stemming_language)
                            if alphanumeric_group:
                                if last_word:
                                    words['%s %s' % (last_word, alphanumeric_group)] = 1
                                last_word = alphanumeric_group
        return words.keys()


    def tokenize_for_phrases(self, phrase):
        """Return list of phrases found in PHRASE.  Note that the phrase is
           split into groups depending on the alphanumeric characters and
           punctuation characters definition present in the config file.
        """
        phrase = wash_for_utf8(phrase)
        return [phrase]



class BibIndexExactAuthorTokenizer(BibIndexDefaultTokenizer):
    """
    Human name exact tokenizer.
    Old: BibIndexExactNameTokenizer
    """
    def __init__(self, stemming_language = None, remove_stopwords = 'No', remove_html_markup = 'No', remove_latex_markup = 'No'):
        BibIndexDefaultTokenizer.__init__(self, stemming_language,
                                                remove_stopwords,
                                                remove_html_markup,
                                                remove_latex_markup)

    def tokenize_for_phrases(self, s):
        """
        Returns washed autor name.
        """
        return [wash_author_name(s)]



class BibIndexAuthorTokenizer(BibIndexDefaultTokenizer):
    """Human name tokenizer.

    Human names are divided into three classes of tokens:
    'lastnames', i.e., family, tribal or group identifiers,
    'nonlastnames', i.e., personal names distinguishing individuals,
    'titles', both incidental and permanent, e.g., 'VIII', '(ed.)', 'Msc'
    """

    def __init__(self, stemming_language = None, remove_stopwords = 'No', remove_html_markup = 'No', remove_latex_markup = 'No'):
        BibIndexDefaultTokenizer.__init__(self, stemming_language,
                                                remove_stopwords,
                                                remove_html_markup,
                                                remove_latex_markup)
        self.single_initial_re = re.compile('^\w\.$')
        self.split_on_re = re.compile('[\.\s-]')
        # lastname_stopwords describes terms which should not be used for indexing,
        # in multiple-word last names.  These are purely conjunctions, serving the
        # same function as the American hyphen, but using linguistic constructs.
        self.lastname_stopwords = set(['y', 'of', 'and', 'de'])

    def scan_string_for_phrases(self, s):
        """Scan a name string and output an object representing its structure.

        @param s: the input to be lexically tagged
        @type s: string

        @return: dict of lexically tagged input items.

            Sample output for the name 'Jingleheimer Schmitt, John Jacob, XVI.' is:
            {
                'TOKEN_TAG_LIST' : ['lastnames', 'nonlastnames', 'titles', 'raw'],
                'lastnames'      : ['Jingleheimer', 'Schmitt'],
                'nonlastnames'   : ['John', 'Jacob'],
                'titles'         : ['XVI.'],
                'raw'            : 'Jingleheimer Schmitt, John Jacob, XVI.'
            }
        @rtype: dict
        """
        retval = {'TOKEN_TAG_LIST' : ['lastnames', 'nonlastnames', 'titles', 'raw'],
                  'lastnames'      : [],
                  'nonlastnames'   : [],
                  'titles'         : [],
                  'raw'            : s}
        l = s.split(',')
        if len(l) < 2:
            # No commas means a simple name
            new = s.strip()
            new = s.split(' ')
            if len(new) == 1:
                retval['lastnames'] = new        # rare single-name case
            else:
                retval['lastnames'] = new[-1:]
                retval['nonlastnames'] = new[:-1]
                for tag in ['lastnames', 'nonlastnames']:
                    retval[tag] = [x.strip() for x in retval[tag]]
                    retval[tag] = [re.split(self.split_on_re, x) for x in retval[tag]]
                        # flatten sublists
                    retval[tag] = [item for sublist in retval[tag] for item in sublist]
                    retval[tag] = [x for x in retval[tag] if x != '']
        else:
            # Handle lastname-first multiple-names case
            retval['titles'] = l[2:]             # no titles? no problem
            retval['nonlastnames'] = l[1]
            retval['lastnames'] = l[0]
            for tag in ['lastnames', 'nonlastnames']:
                retval[tag] = retval[tag].strip()
                retval[tag] = re.split(self.split_on_re, retval[tag])
                    # filter empty strings
                retval[tag] = [x for x in retval[tag] if x != '']
            retval['titles'] = [x.strip() for x in retval['titles'] if x != '']

        return retval

    def parse_scanned_for_phrases(self, scanned):
        """Return all the indexable variations for a tagged token dictionary.

        Does this via the combinatoric expansion of the following rules:
        - Expands first names as name, first initial with period, first initial
            without period.
        - Expands compound last names as each of their non-stopword subparts.
        - Titles are treated literally, but applied serially.

        Please note that titles will be applied to complete last names only.
        So for example, if there is a compound last name of the form,
        "Ibanez y Gracia", with the title, "(ed.)", then only the combination
        of those two strings will do, not "Ibanez" and not "Gracia".

        @param scanned: lexically tagged input items in the form of the output
            from scan()
        @type scanned: dict

        @return: combinatorically expanded list of strings for indexing
        @rtype: list of string
        """

        def _fully_expanded_last_name(first, lastlist, title = None):
            """Return a list of all of the first / last / title combinations.

            @param first: one possible non-last name
            @type first: string

            @param lastlist: the strings of the tokens in the (possibly compound) last name
            @type lastlist: list of string

            @param title: one possible title
            @type title: string
            """
            retval = []
            title_word = ''
            if title != None:
                title_word = ', ' + title

            last = ' '.join(lastlist)
            retval.append(first + ' ' + last + title_word)
            retval.append(last + ', ' + first + title_word)
            for last in lastlist:
                if last in self.lastname_stopwords:
                    continue
                retval.append(first + ' ' + last + title_word)
                retval.append(last + ', ' + first + title_word)

            return retval

        last_parts = scanned['lastnames']
        first_parts = scanned['nonlastnames']
        titles = scanned['titles']
        raw = scanned['raw']

        if len(first_parts) == 0:                       # rare single-name case
            return scanned['lastnames']

        expanded = []
        for exp in self.__expand_nonlastnames(first_parts):
            expanded.extend(_fully_expanded_last_name(exp, last_parts, None))
            for title in titles:
                # Drop titles which are parenthesized.  This eliminates (ed.) from the index, but
                # leaves XI, for example.  This gets rid of the surprising behavior that searching
                # for 'author:ed' retrieves people who have been editors, but whose names aren't
                # Ed.
                # TODO: Make editorship and other special statuses a MARC field.
                if title.find('(') != -1:
                    continue
                # XXX: remember to document that titles can only be applied to complete last names
                expanded.extend(_fully_expanded_last_name(exp, [' '.join(last_parts)], title))

        return sorted(list(set(expanded)))

    def __expand_nonlastnames(self, namelist):
        """Generate every expansion of a series of human non-last names.

        Example:
        "Michael Edward" -> "Michael Edward", "Michael E.", "Michael E", "M. Edward", "M Edward",
                            "M. E.", "M. E", "M E.", "M E", "M.E."
                    ...but never:
                    "ME"

        @param namelist: a collection of names
        @type namelist: list of string

        @return: a greatly expanded collection of names
        @rtype: list of string
        """

        def _expand_name(name):
            """Lists [name, initial, empty]"""
            if name == None:
                return []
            return [name, name[0]]

        def _pair_items(head, tail):
            """Lists every combination of head with each and all of tail"""
            if len(tail) == 0:
                return [head]
            l = []
            l.extend([head + ' ' + tail[0]])
            #l.extend([head + '-' + tail[0]])
            l.extend(_pair_items(head, tail[1:]))
            return l

        def _collect(head, tail):
            """Brings together combinations of things"""

            def _cons(a, l):
                l2 = l[:]
                l2.insert(0, a)
                return l2

            if len(tail) == 0:
                return [head]
            l = []
            l.extend(_pair_items(head, _expand_name(tail[0])))
            l.extend([' '.join(_cons(head, tail)).strip()])
            #l.extend(['-'.join(_cons(head, tail)).strip()])
            l.extend(_collect(head, tail[1:]))
            return l

        def _expand_contract(namelist):
            """Runs collect with every head in namelist and its tail"""
            val = []
            for i  in range(len(namelist)):
                name = namelist[i]
                for expansion in _expand_name(name):
                    val.extend(_collect(expansion, namelist[i+1:]))
            return val

        def _add_squashed(namelist):
            """Finds cases like 'M. E.' and adds 'M.E.'"""
            val = namelist

            def __check_parts(parts):
                if len(parts) < 2:
                    return False
                for part in parts:
                    if not self.single_initial_re.match(part):
                        return False
                return True

            for name in namelist:
                parts = name.split(' ')
                if not __check_parts(parts):
                    continue
                val.extend([''.join(parts)])

            return val

        return _add_squashed(_expand_contract(namelist))


    def tokenize_for_fuzzy_authors(self, phrase):
        """Output the list of strings expanding phrase.

        Does this via the combinatoric expansion of the following rules:
        - Expands first names as name, first initial with period, first initial
            without period.
        - Expands compound last names as each of their non-stopword subparts.
        - Titles are treated literally, but applied serially.

        Please note that titles will be applied to complete last names only.
        So for example, if there is a compound last name of the form,
        "Ibanez y Gracia", with the title, "(ed.)", then only the combination
        of those two strings will do, not "Ibanez" and not "Gracia".

        Old: BibIndexFuzzyAuthorTokenizer

        @param phrase: the input to be lexically tagged
        @type phrase: string

        @return: combinatorically expanded list of strings for indexing
        @rtype: list of string

        @note: A simple wrapper around scan and parse_scanned.
        """
        return self.parse_scanned_for_phrases(self.scan_string_for_phrases(phrase))


    def tokenize_for_phrases(self, phrase):
        """
            Another name for tokenize_for_fuzzy_authors.
            It's for the compatibility.
            See: tokenize_for_fuzzy_authors
        """
        return self.tokenize_for_fuzzy_authors(phrase)


    def tokenize_for_words_default(self, phrase):
        """Default tokenize_for_words inherited from default tokenizer"""
        return super(BibIndexAuthorTokenizer, self).tokenize_for_words(phrase)


    def get_author_family_name_words_from_phrase(self, phrase):
        """ Return list of words from author family names, not his/her first names.

        The phrase is assumed to be the full author name.  This is
        useful for CFG_BIBINDEX_AUTHOR_WORD_INDEX_EXCLUDE_FIRST_NAMES.

        @param phrase: phrase to get family name from
        """
        d_family_names = {}
        # first, treat everything before first comma as surname:
        if ',' in phrase:
            d_family_names[phrase.split(',', 1)[0]] = 1
        # second, try fuzzy author tokenizer to find surname variants:
        for name in self.tokenize_for_phrases(phrase):
            if ',' in name:
                d_family_names[name.split(',', 1)[0]] = 1
        # now extract words from these surnames:
        d_family_names_words = {}
        for family_name in d_family_names.keys():
            for word in self.tokenize_for_words_default(family_name):
                d_family_names_words[word] = 1
        return d_family_names_words.keys()


    def tokenize_for_words(self, phrase):
        """
            If CFG_BIBINDEX_AUTHOR_WORD_INDEX_EXCLUDE_FIRST_NAMES is 1 we tokenize only for family names.
            In other case we perform standard tokenization for words.
        """
        if CFG_BIBINDEX_AUTHOR_WORD_INDEX_EXCLUDE_FIRST_NAMES:
            return self.get_author_family_name_words_from_phrase(phrase)
        else:
            return self.tokenize_for_words_default(phrase)



class BibIndexYearTokenizer(BibIndexDefaultTokenizer):
    """
       Year tokenizer. It tokenizes words from date tags or uses default word tokenizer.
    """

    def __init__(self, stemming_language = None, remove_stopwords = 'No', remove_html_markup = 'No', remove_latex_markup = 'No'):
        BibIndexDefaultTokenizer.__init__(self, stemming_language,
                                                remove_stopwords,
                                                remove_html_markup,
                                                remove_latex_markup)


    def get_words_from_date_tag(self, datestring):
        """
        Special procedure to index words from tags storing date-like
        information in format YYYY or YYYY-MM or YYYY-MM-DD.  Namely, we
        are indexing word-terms YYYY, YYYY-MM, YYYY-MM-DD, but never
        standalone MM or DD.
        """
        out = []
        for dateword in datestring.split():
            # maybe there are whitespaces, so break these too
            out.append(dateword)
            parts = dateword.split('-')
            for nb in range(1, len(parts)):
                out.append("-".join(parts[:nb]))
        return out


    def tokenize_for_words_default(self, phrase):
        """Default tokenize_for_words inherited from default tokenizer"""
        return super(BibIndexYearTokenizer, self).tokenize_for_words(phrase)


    def tokenize_for_words(self, phrase):
        """
            If CFG_INSPIRE_SITE is 1 we perform special tokenization which relies on getting words form date tag.
            In other case we perform default tokenization.
        """
        if CFG_INSPIRE_SITE:
            return self.get_words_from_date_tag(phrase)
        else:
            return self.tokenize_for_words_default(phrase)


class BibIndexFulltextTokenizer(BibIndexDefaultTokenizer):
    """
        Exctracts all the words contained in document specified by url.
    """

    def __init__(self, stemming_language = None, remove_stopwords = 'No', remove_html_markup = 'No', remove_latex_markup = 'No'):
        self.verbose = 3
        BibIndexDefaultTokenizer.__init__(self, stemming_language,
                                                remove_stopwords,
                                                remove_html_markup,
                                                remove_latex_markup)

    def set_verbose(self, verbose):
        """Allows to change verbosity level during indexing"""
        self.verbose = verbose

    def tokenize_for_words_default(self, phrase):
        """Default tokenize_for_words inherited from default tokenizer"""
        return super(BibIndexFulltextTokenizer, self).tokenize_for_words(phrase)


    def get_words_from_fulltext(self, url_direct_or_indirect):
        """Returns all the words contained in the document specified by
           URL_DIRECT_OR_INDIRECT with the words being split by various
           SRE_SEPARATORS regexp set earlier.  If FORCE_FILE_EXTENSION is
           set (e.g. to "pdf", then treat URL_DIRECT_OR_INDIRECT as a PDF
           file.  (This is interesting to index Indico for example.)  Note
           also that URL_DIRECT_OR_INDIRECT may be either a direct URL to
           the fulltext file or an URL to a setlink-like page body that
           presents the links to be indexed.  In the latter case the
           URL_DIRECT_OR_INDIRECT is parsed to extract actual direct URLs
           to fulltext documents, for all knows file extensions as
           specified by global CONV_PROGRAMS config variable.
        """
        write_message("... reading fulltext files from %s started" % url_direct_or_indirect, verbose=2)
        try:
            if bibdocfile_url_p(url_direct_or_indirect):
                write_message("... %s is an internal document" % url_direct_or_indirect, verbose=2)
                bibdoc = bibdocfile_url_to_bibdoc(url_direct_or_indirect)
                indexer = get_idx_indexer('fulltext')
                if indexer != 'native':
                    # A document might belong to multiple records
                    for rec_link in bibdoc.bibrec_links:
                        recid = rec_link["recid"]
                        # Adds fulltexts of all files once per records
                        if not recid in fulltext_added:
                            bibrecdocs = BibRecDocs(recid)
                            text = bibrecdocs.get_text()
                            if indexer == 'SOLR' and CFG_SOLR_URL:
                                solr_add_fulltext(recid, text)
                            elif indexer == 'XAPIAN' and CFG_XAPIAN_ENABLED:
                                xapian_add(recid, 'fulltext', text)

                        fulltext_added.add(recid)
                    # we are relying on an external information retrieval system
                    # to provide full-text indexing, so dispatch text to it and
                    # return nothing here:
                    return []
                else:
                    text = ""
                    if hasattr(bibdoc, "get_text"):
                        text = bibdoc.get_text()
                    return self.tokenize_for_words_default(text)
            else:
                if CFG_BIBINDEX_FULLTEXT_INDEX_LOCAL_FILES_ONLY:
                    write_message("... %s is external URL but indexing only local files" % url_direct_or_indirect, verbose=2)
                    return []
                write_message("... %s is an external URL" % url_direct_or_indirect, verbose=2)
                urls_to_index = set()
                for splash_re, url_re in CFG_BIBINDEX_SPLASH_PAGES.iteritems():
                    if re.match(splash_re, url_direct_or_indirect):
                        write_message("... %s is a splash page (%s)" % (url_direct_or_indirect, splash_re), verbose=2)
                        html = urllib2.urlopen(url_direct_or_indirect).read()
                        urls = get_links_in_html_page(html)
                        write_message("... found these URLs in %s splash page: %s" % (url_direct_or_indirect, ", ".join(urls)), verbose=3)
                        for url in urls:
                            if re.match(url_re, url):
                                write_message("... will index %s (matched by %s)" % (url, url_re), verbose=2)
                                urls_to_index.add(url)
                if not urls_to_index:
                    urls_to_index.add(url_direct_or_indirect)
                write_message("... will extract words from %s" % ', '.join(urls_to_index), verbose=2)
                words = {}
                for url in urls_to_index:
                    tmpdoc = download_url(url)
                    file_converter_logger = get_file_converter_logger()
                    old_logging_level = file_converter_logger.getEffectiveLevel()
                    if self.verbose > 3:
                        file_converter_logger.setLevel(logging.DEBUG)
                    try:
                        try:
                            tmptext = convert_file(tmpdoc, output_format='.txt')
                            text = open(tmptext).read()
                            os.remove(tmptext)

                            indexer = get_idx_indexer('fulltext')
                            if indexer != 'native':
                                if indexer == 'SOLR' and CFG_SOLR_URL:
                                    solr_add_fulltext(None, text) # FIXME: use real record ID
                                if indexer == 'XAPIAN' and CFG_XAPIAN_ENABLED:
                                    #xapian_add(None, 'fulltext', text) # FIXME: use real record ID
                                    pass
                                # we are relying on an external information retrieval system
                                # to provide full-text indexing, so dispatch text to it and
                                # return nothing here:
                                tmpwords = []
                            else:
                                tmpwords = self.tokenize_for_words_default(text)
                            words.update(dict(map(lambda x: (x, 1), tmpwords)))
                        except Exception, e:
                            message = 'ERROR: it\'s impossible to correctly extract words from %s referenced by %s: %s' % (url, url_direct_or_indirect, e)
                            register_exception(prefix=message, alert_admin=True)
                            write_message(message, stream=sys.stderr)
                    finally:
                        os.remove(tmpdoc)
                        if self.verbose > 3:
                            file_converter_logger.setLevel(old_logging_level)
                return words.keys()
        except Exception, e:
            message = 'ERROR: it\'s impossible to correctly extract words from %s: %s' % (url_direct_or_indirect, e)
            register_exception(prefix=message, alert_admin=True)
            write_message(message, stream=sys.stderr)
            return []


    def tokenize_for_words(self, phrase):
        return self.get_words_from_fulltext(phrase)


class BibIndexJournalTokenizer(BibIndexEmptyTokenizer):
    """
        Tokenizer for journal index. It returns joined title/volume/year/page as a word from journal tag.
        (In fact it's an aggregator.)
    """

    def __init__(self, stemming_language = None, remove_stopwords = 'No', remove_html_markup = 'No', remove_latex_markup = 'No'):
        self.tag = CFG_JOURNAL_TAG
        self.journal_pubinfo_standard_form = CFG_JOURNAL_PUBINFO_STANDARD_FORM
        self.journal_pubinfo_standard_form_regexp_check = CFG_JOURNAL_PUBINFO_STANDARD_FORM_REGEXP_CHECK


    def tokenize(self, recID):
        """
        Special procedure to extract words from journal tags.  Joins
        title/volume/year/page into a standard form that is also used for
        citations.
        """
        # get all journal tags/subfields:
        bibXXx = "bib" + self.tag[0] + self.tag[1] + "x"
        bibrec_bibXXx = "bibrec_" + bibXXx
        query = """SELECT bb.field_number,b.tag,b.value FROM %s AS b, %s AS bb
                    WHERE bb.id_bibrec=%%s
                      AND bb.id_bibxxx=b.id AND tag LIKE %%s""" % (bibXXx, bibrec_bibXXx)
        res = run_sql(query, (recID, self.tag))
        # construct journal pubinfo:
        dpubinfos = {}
        for row in res:
            nb_instance, subfield, value = row
            if subfield.endswith("c"):
                # delete pageend if value is pagestart-pageend
                # FIXME: pages may not be in 'c' subfield
                value = value.split('-', 1)[0]
            if dpubinfos.has_key(nb_instance):
                dpubinfos[nb_instance][subfield] = value
            else:
                dpubinfos[nb_instance] = {subfield: value}

        # construct standard format:
        lwords = []
        for dpubinfo in dpubinfos.values():
            # index all journal subfields separately
            for tag, val in dpubinfo.items():
                lwords.append(val)
            # index journal standard format:
            pubinfo = self.journal_pubinfo_standard_form
            for tag, val in dpubinfo.items():
                pubinfo = pubinfo.replace(tag, val)
            if self.tag[:-1] in pubinfo:
                # some subfield was missing, do nothing
                pass
            else:
                lwords.append(pubinfo)

        # return list of words and pubinfos:
        return lwords

    def get_function_by_type(self, wordtable_type):
        return self.tokenize


class BibIndexAuthorCountTokenizer(BibIndexEmptyTokenizer):
    """
        Returns a number of authors who created a publication with given recID in the database.
    """

    def __init__(self, stemming_language = None, remove_stopwords = 'No', remove_html_markup = 'No', remove_latex_markup = 'No'):
        self.tags = ['100__a', '700__a']


    def tokenize(self, recID):
        """Uses get_field_count from bibindex_engine_utils
           for finding a number of authors of a publication and pass it in the list"""
        return [str(get_field_count(recID, self.tags)),]


    def get_function_by_type(self, wordtable_type):
        return self.tokenize


